#Setup Instructions
----------------
#To run a standalone:
--Setup Ubuntu/Centos instances
--Install java11 via 'sudo apt install openjdk-11-jdk' or java 8
--Check python version, preferably above 3.6 say python3.8

sudo apt update

--update .bashrc for user
alias python=python3.8
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/
export PATH=$PATH:$JAVA_HOME/bin
#export AIRFLOW_HOME=<airflow_path>

--refresh
source .bashrc

--other packages which ay be required/helpful
sudo apt install git -y
sudo apt install vim -y
sudo apt install openssh-server -y
sudo apt install wget -y
sudo apt install python3-pip

--Install sqlite which will be required when we initialize airflow for first time.(sqllite for backend db)
sudo apt install sqlite3

--Install venv for virtual env (if we want to use it for our work)
sudo apt-get install python3.8-venv

hdu@mh3:~$ python -m venv my-venv
hdu@mh3:~$ source my-venv/bin/activate
(my-venv) hdu@mh3:~$ pwd
/home/hdu

(my-venv) hdu@mh3:~$ sudo apt install libpq-dev

(my-venv) hdu@mh3:~$ pip install 'apache-airflow[postgres]==2.8.1' \
>  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.8.1/constraints-3.8.txt"

Commands:
airflow -h
airflow standalone
airflow info
airflow standalone

(my-venv) hdu@mh3:~$ airflow -h
Usage: airflow [-h] GROUP_OR_COMMAND ...

--to test standalone and also let it created default configs such as airflow.cfg , airflow.db in /home/xxxx/my-venv/
(my-venv) hdu@mh3:~$ airflow standalone

--if all fine, cancel the standalone ..

--Initialize db

make sure, airflow.cfg in /home/xxxx/my-venv/airflow.cfg is edited to have load_examples=False

(my-venv) hdu@mh3:~$ airflow db init
/home/hdu/my-venv/lib/python3.8/site-packages/airflow/cli/commands/db_command.py:47 DeprecationWarning: `db init` is deprecated.  Use `db migrate` instead to migrate the db and/or airflow connections create-default-connections to create the default connections
DB: sqlite:////home/hdu/airflow/airflow.db
[2024-11-17T18:00:33.847+0100] {migration.py:216} INFO - Context impl SQLiteImpl.
[2024-11-17T18:00:33.848+0100] {migration.py:219} INFO - Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running stamp_revision  -> 88344c1d9134
WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.
Initialization done

--Install postgres as that's what we want to use.
Using sqlite3 restricts us in context of running multiple tasks.

(my-venv) hdu@mh3:~$  sudo apt-get install postgresql postgresql-contrib

----output-------
..
Success. You can now start the database server using:

    pg_ctlcluster 12 main start

Ver Cluster Port Status Owner    Data directory              Log file
12  main    5432 down   postgres /var/lib/postgresql/12/main /var/log/postgresql/postgresql-12-main.log

----output ends-------
Connect to postgres..
(my-venv) hdu@mh3:~$ sudo -i -u postgres

postgres@mh3:~$ psql
psql (12.20 (Ubuntu 12.20-0ubuntu0.20.04.1))
Type "help" for help.

postgres=#

postgres=#CREATE DATABASE airflow;
CREATE USER airflow WITH PASSWORD 'airflow';
GRANT ALL PRIVILEGES ON DATABASE airflow to airflow;

<ctrl-d> twice to exit and back into my-venv.

--Configure airflow to connect to postgres instead of default sqlite3

(my-venv) hdu@mh1:~$ ls my-venv/
airflow.cfg  airflow.db  airflow-webserver.pid  bin  dags  include  lib  lib64  logs  pyvenv.cfg  share  standalone_admin_password.txt  webserver_config.py

Edit airflow.cfg file to point to postgres..
(my-venv) hdu@mh3:~$ sed -i 's#sqlite:////home/hdu/my-venv/airflow.db#postgresql+psycopg2://airflow:airflow@localhost/airflow#g' /home/hdu/my-venv/airflow.cfg 

--to check conn details
(my-venv) hdu@mh3:~$ grep sql_alchemy /home/hdu/my-venv/airflow.cfg
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
# Example: sql_alchemy_engine_args = {"arg1": True}
# sql_alchemy_engine_args = 
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True
sql_alchemy_schema = 
# Example: sql_alchemy_connect_args = {"timeout": 30}
# sql_alchemy_connect_args = 

--check executor
(my-venv) hdu@mh3:~$ grep executor /home/hdu/my-venv/airflow.cfg
# The executor class that airflow should use. Choices include
# full import path to the class when using a custom executor.
executor = SequentialExecutor
# start date from stealing all the executor slots in a cluster.
# Collation for ``dag_id``, ``task_id``, ``key``, ``external_executor_id`` columns
# the scheduler, executor, or callback execution context. This can help in circumstances such as
# with the elements of the list (e.g: "scheduler,executor,dagrun")
# the elements of the list (e.g: "scheduler,executor,dagrun").

NOte** Tasks can only be executed sequentially
--change it
(my-venv) hdu@mh3:~$ sed -i 's#SequentialExecutor#LocalExecutor#g' /home/hdu/my-venv/airflow.cfg

(my-venv) hdu@mh3:~$ grep executor /home/hdu/my-venv/airflow.cfg
# The executor class that airflow should use. Choices include
# full import path to the class when using a custom executor.
executor = LocalExecutor
# start date from stealing all the executor slots in a cluster.
# Collation for ``dag_id``, ``task_id``, ``key``, ``external_executor_id`` columns
# the scheduler, executor, or callback execution context. This can help in circumstances such as
# with the elements of the list (e.g: "scheduler,executor,dagrun")
# the elements of the list (e.g: "scheduler,executor,dagrun").

--initialize airflow again
(my-venv) hdu@mh3:~$ airflow db init
/home/hdu/my-venv/lib/python3.8/site-packages/airflow/cli/commands/db_command.py:47 DeprecationWarning: `db init` is deprecated.  Use `db migrate` instead to migrate the db and/or airflow connections create-default-connections to create the default connections
DB: postgresql+psycopg2://airflow:***@localhost/airflow
[2024-11-18T09:45:30.216+0100] {migration.py:216} INFO - Context impl PostgresqlImpl.
[2024-11-18T09:45:30.226+0100] {migration.py:219} INFO - Will assume transactional DDL.
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running stamp_revision  -> 88344c1d9134
WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.
Initialization done

--create user
airflow users create -u airflow -f airflow -l airflow -r Admin -e airflow@gmail.com
pswd: abcd@1234

--to run webserver in background
(my-venv) hdu@mh3:~$ nohup airflow webserver &
[1] 8170

Note** it throws exception about connection in use or port in use, we can check processes that are zombie or running & kill them or start new processes with new ports, 
FOR EXAMPLE: nohup airflow webserver -p 8081 &

---sample output------
(my-venv) hdu@mh3:~$ [2024-11-18T09:48:21.666+0100] {configuration.py:2065} INFO - Creating new FAB webserver config file in: /home/hdu/airflow/webserver_config.py
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
Running the Gunicorn Server with:
Workers: 4 sync
Host: 0.0.0.0:8080
Timeout: 120
Logfiles: - -
Access Logformat: 
=================================================================
/home/hdu/my-venv/lib/python3.8/site-packages/flask_limiter/extension.py:336 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
[2024-11-18T09:48:27.589+0100] {override.py:1769} INFO - Created Permission View: can create on DAG Runs
[2024-11-18T09:48:27.614+0100] {override.py:1820} INFO - Added Permission can create on DAG Runs to role Admin
[2024-11-18T09:48:27.646+0100] {override.py:1769} INFO - Created Permission View: can read on DAG Runs
[2024-11-18T09:48:32.406+0100] {override.py:1769} INFO - Created Permission View: menu access on Cluster Activity
[2024-11-18T09:48:32.434+0100] {override.py:1820} INFO - Added Permission menu access on Cluster Activity to role Admin
[2024-11-18T09:48:32.472+0100] {override.py:1769} INFO - Created Permission View: menu access on Datasets
[2024-11-18T09:48:32.529+0100] {override.py:1820} INFO - Added Permission menu access on Datasets to role Admin
...
[2024-11-18T09:48:32.568+0100] {override.py:1769} INFO - Created Permission View: menu access on Documentation
[2024-11-18T09:48:32.604+0100] {override.py:1820} INFO - Added Permission menu access on Documentation to role Admin
[2024-11-18T09:48:32.666+0100] {override.py:1769} INFO - Created Permission View: menu access on Docs
[2024-11-18T09:48:32.698+0100] {override.py:1820} INFO - Added Permission menu access on Docs to role Admin
[2024-11-18T09:48:35.135+0100] {options.py:83} WARNING - The swagger_ui directory could not be found.
    Please install connexion with extra install: pip install connexion[swagger-ui]
    or provide the path to your local installation by passing swagger_path=<your path>

[2024-11-18T09:48:35.136+0100] {options.py:83} WARNING - The swagger_ui directory could not be found.
    Please install connexion with extra install: pip install connexion[swagger-ui]
    or provide the path to your local installation by passing swagger_path=<your path>

[2024-11-18T09:48:35.222+0100] {override.py:1769} INFO - Created Permission View: can edit on DAGs
[2024-11-18T09:48:35.249+0100] {override.py:1769} INFO - Created Permission View: can delete on DAGs
[2024-11-18T09:48:35.282+0100] {override.py:1769} INFO - Created Permission View: can read on DAGs
[2024-11-18T09:48:35.359+0100] {override.py:1369} INFO - Inserted Role: Viewer
.........
[2024-11-18T09:48:37.159+0100] {override.py:1820} INFO - Added Permission can read on DAG Dependencies to role Admin
[2024-11-18T09:48:37.186+0100] {override.py:1820} INFO - Added Permission can read on DAG Code to role Admin
[2024-11-18T09:48:37.205+0100] {override.py:1820} INFO - Added Permission can read on Datasets to role Admin
[2024-11-18T09:48:37.235+0100] {override.py:1820} INFO - Added Permission can read on Cluster Activity to role Admin
[2024-11-18T09:48:37.254+0100] {override.py:1820} INFO - Added Permission can read on ImportError to role Admin
[2024-11-18T09:48:37.285+0100] {override.py:1820} INFO - Added Permission can read on DAG Warnings to role Admin
[2024-11-18T09:48:37.310+0100] {override.py:1820} INFO - Added Permission can read on Task Logs to role Admin
[2024-11-18T09:48:37.343+0100] {override.py:1820} INFO - Added Permission can read on Website to role Admin
[2024-11-18T09:48:37.368+0100] {override.py:1820} INFO - Added Permission can edit on DAGs to role Admin
[2024-11-18T09:48:37.374+0100] {override.py:1820} INFO - Added Permission can delete on DAGs to role Admin
[2024-11-18 09:48:37 +0100] [8173] [INFO] Starting gunicorn 21.2.0
[2024-11-18 09:48:37 +0100] [8173] [INFO] Listening at: http://0.0.0.0:8080 (8173)
[2024-11-18 09:48:37 +0100] [8173] [INFO] Using worker: sync
[2024-11-18 09:48:37 +0100] [8177] [INFO] Booting worker with pid: 8177
[2024-11-18 09:48:37 +0100] [8178] [INFO] Booting worker with pid: 8178
[2024-11-18 09:48:37 +0100] [8179] [INFO] Booting worker with pid: 8179
[2024-11-18 09:48:37 +0100] [8180] [INFO] Booting worker with pid: 8180

--run scheduler
(my-venv) hdu@mh3:~$ nohup airflow scheduler &
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-11-18T09:50:12.546+0100] {task_context_logger.py:63} INFO - Task context logging is enabled
[2024-11-18T09:50:12.548+0100] {executor_loader.py:115} INFO - Loaded executor: LocalExecutor
[2024-11-18 09:50:12 +0100] [8190] [INFO] Starting gunicorn 21.2.0
[2024-11-18 09:50:12 +0100] [8190] [INFO] Listening at: http://[::]:8793 (8190)
[2024-11-18 09:50:12 +0100] [8190] [INFO] Using worker: sync
[2024-11-18T09:50:12.785+0100] {scheduler_job_runner.py:808} INFO - Starting the scheduler
[2024-11-18T09:50:12.787+0100] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
[2024-11-18 09:50:12 +0100] [8191] [INFO] Booting worker with pid: 8191
[2024-11-18 09:50:12 +0100] [8192] [INFO] Booting worker with pid: 8192
[2024-11-18T09:50:13.936+0100] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 8328
[2024-11-18T09:50:13.960+0100] {scheduler_job_runner.py:1619} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-11-18T09:50:14.025+0100] {settings.py:60} INFO - Configured default timezone UTC

Access from browser
localhost:8080
username: airflow
pswd: abcd@1234


--------------------------------------------------------
Starting...

--we can even test airflow standalone with new configuration..
hdu@mh1:~$ airflow standalone
------sample output-----
standalone | Starting Airflow Standalone
standalone | Checking database is initialized
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
WARNI [unusual_prefix_b640e0e884ae8145646e4eb0bed05093a1dabf36_example_local_kubernetes_executor] Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/home/hdu/.local/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
WARNI [unusual_prefix_b640e0e884ae8145646e4eb0bed05093a1dabf36_example_local_kubernetes_executor] Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
WARNI [unusual_prefix_1607d6d6df7596b1233d110ba20cbc9d50977527_example_kubernetes_executor] The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.
standalone | Database ready
/home/hdu/.local/lib/python3.8/site-packages/flask_limiter/extension.py:336 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
triggerer  | ____________       _____________
triggerer  | ____    |__( )_________  __/__  /________      __
triggerer  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
triggerer  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
triggerer  | _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
triggerer  | [2024-01-23 19:18:54 +0530] [126401] [INFO] Starting gunicorn 21.2.0
triggerer  | [2024-01-23 19:18:54 +0530] [126401] [INFO] Listening at: http://[::]:8794 (126401)
triggerer  | [2024-01-23 19:18:54 +0530] [126401] [INFO] Using worker: sync
triggerer  | [2024-01-23 19:18:54 +0530] [126402] [INFO] Booting worker with pid: 126402
triggerer  | [2024-01-23 19:18:54 +0530] [126403] [INFO] Booting worker with pid: 126403
scheduler  | ____________       _____________
scheduler  | ____    |__( )_________  __/__  /________      __
scheduler  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
scheduler  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
scheduler  | _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
scheduler  | [2024-01-23T19:18:55.982+0530] {task_context_logger.py:63} INFO - Task context logging is enabled
scheduler  | [2024-01-23T19:18:56.003+0530] {executor_loader.py:115} INFO - Loaded executor: SequentialExecutor
triggerer  | [2024-01-23T19:18:56.119+0530] {triggerer_job_runner.py:174} INFO - Setting up TriggererHandlerWrapper with handler <FileTaskHandler (NOTSET)>
triggerer  | [2024-01-23T19:18:56.127+0530] {triggerer_job_runner.py:230} INFO - Setting up logging queue listener with handlers [<RedirectStdHandler <stdout> (NOTSET)>, <TriggererHandlerWrapper (NOTSET)>]
triggerer  | [2024-01-23T19:18:56.191+0530] {triggerer_job_runner.py:331} INFO - Starting the triggerer
scheduler  | [2024-01-23T19:18:56.218+0530] {scheduler_job_runner.py:808} INFO - Starting the scheduler
scheduler  | [2024-01-23T19:18:56.223+0530] {scheduler_job_runner.py:815} INFO - Processing each file at most -1 times
scheduler  | [2024-01-23 19:18:56 +0530] [126413] [INFO] Starting gunicorn 21.2.0
scheduler  | [2024-01-23T19:18:56.282+0530] {manager.py:169} INFO - Launched DagFileProcessorManager with pid: 126415
scheduler  | [2024-01-23T19:18:56.292+0530] {scheduler_job_runner.py:1619} INFO - Adopting or resetting orphaned tasks for active dag runs
scheduler  | [2024-01-23T19:18:56.300+0530] {settings.py:60} INFO - Configured default timezone UTC
scheduler  | [2024-01-23 19:18:56 +0530] [126413] [INFO] Listening at: http://[::]:8793 (126413)
scheduler  | [2024-01-23 19:18:56 +0530] [126413] [INFO] Using worker: sync
scheduler  | [2024-01-23 19:18:56 +0530] [126416] [INFO] Booting worker with pid: 126416
scheduler  | [2024-01-23 19:18:56 +0530] [126417] [INFO] Booting worker with pid: 126417
triggerer  | [2024-01-23T19:18:56.750+0530] {triggerer_job_runner.py:576} INFO - Triggerer's async thread was blocked for 0.43 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
scheduler  | [2024-01-23T19:18:56.890+0530] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
webserver  | /home/hdu/.local/lib/python3.8/site-packages/flask_limiter/extension.py:336 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
webserver  | [2024-01-23 19:19:03 +0530] [126400] [INFO] Starting gunicorn 21.2.0
webserver  | [2024-01-23 19:19:03 +0530] [126400] [INFO] Listening at: http://0.0.0.0:8080 (126400)
webserver  | [2024-01-23 19:19:03 +0530] [126400] [INFO] Using worker: sync
webserver  | [2024-01-23 19:19:03 +0530] [126426] [INFO] Booting worker with pid: 126426
webserver  | [2024-01-23 19:19:03 +0530] [126427] [INFO] Booting worker with pid: 126427
webserver  | [2024-01-23 19:19:03 +0530] [126428] [INFO] Booting worker with pid: 126428
webserver  | [2024-01-23 19:19:03 +0530] [126429] [INFO] Booting worker with pid: 126429
webserver  | 127.0.0.1 - - [23/Jan/2024:19:19:04 +0530] "GET /home HTTP/1.1" 200 389902 "http://localhost:8080/configuration" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0"
(X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0"
webserver  | 127.0.0.1 - - [23/Jan/2024:19:20:24 +0530] "POST /last_dagruns HTTP/1.1" 200 2 "http://localhost:8080/home" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0"
webserver  | 127.0.0.1 - - [23/Jan/2024:19:20:24 +0530] "POST /dag_stats HTTP/1.1" 200 8439 "http://localhost:8080/home" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0"
webserver  | 127.0.0.1 - - [23/Jan/2024:19:20:24 +0530] "POST /task_stats HTTP/1.1" 200 27054 "http://localhost:8080/home" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0"
triggerer  | [2024-01-23T19:20:56.387+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running
triggerer  | [2024-01-23T19:21:56.478+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running
triggerer  | [2024-01-23T19:22:56.557+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running
triggerer  | [2024-01-23T19:23:47.535+0530] {triggerer_job_runner.py:576} INFO - Triggerer's async thread was blocked for 0.28 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
triggerer  | [2024-01-23T19:23:56.652+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running
scheduler  | [2024-01-23T19:23:57.050+0530] {scheduler_job_runner.py:1619} INFO - Adopting or resetting orphaned tasks for active dag runs
triggerer  | [2024-01-23T19:24:07.833+0530] {triggerer_job_runner.py:576} INFO - Triggerer's async thread was blocked for 0.31 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
triggerer  | [2024-01-23T19:24:25.265+0530] {triggerer_job_runner.py:576} INFO - Triggerer's async thread was blocked for 0.27 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
triggerer  | [2024-01-23T19:24:56.901+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running
triggerer  | [2024-01-23T19:25:06.357+0530] {triggerer_job_runner.py:576} INFO - Triggerer's async thread was blocked for 0.37 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
triggerer  | [2024-01-23T19:25:56.999+0530] {triggerer_job_runner.py:481} INFO - 0 triggers currently running


------output ends--------

--Access web UI
http://localhost:8080/home

--to disable default DAGs
AIRFLOW_HOME/airflow.cfg 

set AIRFLOW__CORE__LOAD__EXAMPLES: False

restart airflow...

======================================
For vscode..

sudo apt install snapd
snap install code --classic

--------------------------


