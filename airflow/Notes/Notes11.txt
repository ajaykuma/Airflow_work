Using Datasets:
With Datasets, DAGs that access the same data can have explicit, visible relationships, and DAGs can be scheduled based on updates to these datasets. 
This feature helps make Airflow data-aware and expands Airflow scheduling capabilities beyond time-based methods such as cron.

Datasets can help resolve common issues. For example, consider a data engineering team with a DAG that creates a dataset and 
a machine learning team with a DAG that trains a model on the dataset. Using datasets, the machine learning team's DAG runs only when the data engineering team's DAG has produced an update to the dataset.

#Reduce the amount of code necessary to implement cross-DAG dependencies. 
#Get better visibility into how your DAGs are connected and how they depend on data
#Reduce costs, because datasets do not use a worker slot in contrast to sensors or other implementations of cross-DAG dependencies.

#Dataset: an object that is defined by a unique URI. 
#If you want to avoid validity parsing, prefix your dataset name with x- for Airflow to treat it as a string. 
#Dataset event: an event that is attached to a dataset and created whenever a producer task updates 
#Dataset schedule: the schedule of a DAG that is triggered as soon as dataset events for one or more datasets are created.
#Producer task: a task that produces updates to one or more datasets provided to its OUTLETS parameter, creating dataset events when it completes successfully.

#Outlets: a task parameter that contains the list of datasets a specific tasks produces updates to, as soon as it completes successfully. 
Note that Airflow is not yet aware of the underlying data. It is up to you to determine which tasks should be considered producer tasks for a dataset. As long as a task has an outlet dataset, Airflow considers it a producer task even if that task doesn't operate on the referenced dataset.

Inlets: a task parameter that contains the list of datasets a specific task has access to, typically to access extra information from related dataset events. Defining inlets for a task does not affect the schedule of the DAG containing the task and the relationship is not reflected in the Airflow UI.


#Creating dependency between dags based on file ie using a DS
--create producer

my_file = Dataset("/home/hdu/airflow/configs/sample.txt")
# default_args = {

#     'owner': 'hdu',
#     'retries': 5,
#     'retry_delay': timedelta(minutes=5)
# }

with DAG(
    dag_id = "producer",
    schedule='@daily',
    start_date=datetime(2024,11,20),
    catchup=False
):

    @task(outlets=[my_file])
    def the_task_update():
        with open(my_file.uri, "a+") as f:
            f.write("producer update")

    the_task_update()

Note** Needed to run first for other dag ie consumer to get successfully run

--create consumer
my_file = Dataset("/home/hdu/airflow/configs/sample.txt")
# default_args = {

#     'owner': 'hdu',
#     'retries': 5,
#     'retry_delay': timedelta(minutes=5)
# }

with DAG(
    dag_id = "consumer",
    schedule=[my_file],
    start_date=datetime(2024,11,20),
    #dagrun_timeout=timedelta(seconds=10),  
    catchup=False
):

    
    # @task(outlets=[my_file])
    @task()
    def the_task_read():
        with open(my_file.uri, "r") as f:
            print(f.read())

    the_task_read()

--run consumer and this will show no execution or on wait till producer runs.
--trigger producer and check if consumer then runs.

=============================
--Sensor Operator examples

--import
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator

--tasks

tk_brk = BashOperator(
	task_id = 'tk_brk',
	bash_command = 'sleep 10'
	)

is_api_available = HttpSensor(
	task_id = 'is_api_available',
	http_conn_id = 'user_api',
	endpoint = '/api',
	#response_check=lambda response: "mydata" in response.txt,
	poke_interval = 5,
	timeout=20
	)

extract_user = SimpleHttpOperator(
	task_id = 'extract_user',
	http_conn_id = 'user_api',
	endpoint = '/api',
	method = 'GET',
	response_filter = lambda response:json.loads(response.text),
	log_response = True
	)

file_chk = FileSensor(
	task_id = "is_file_avai",
	fs_conn_id = "file_path",
	filepath="mydata.csv",
	poke_interval = 5,
	timeout=20
	)

Note** Create file connection
connection name : fs_conn_id
conn type: File(path)
Extras: {"path":"/<filepath>"}
==========
	
